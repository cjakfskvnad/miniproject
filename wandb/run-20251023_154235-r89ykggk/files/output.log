
Creating environment (sequence length = 10)...
Creating Transformer model...
Creating DQN agent...

Total parameters: 67459
Device: cuda

Starting training for 10000 episodes...
============================================================
Training:   0%|                                                                               | 0/10000 [00:00<?, ?it/s]/workspace/train_dqn.py:300: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /app/pytorch/torch/csrc/utils/tensor_new.cpp:254.)
  state_tensor = torch.tensor([state], dtype=torch.long)

Episode 10/10000
  Avg Reward: 1.10/10
  Avg Accuracy: 11.00%
  Avg Loss: 0.0000
  Epsilon: 1.0000
  Buffer Size: 21

Episode 20/10000
  Avg Reward: 1.60/10
  Avg Accuracy: 16.00%
  Avg Loss: 0.0000
  Epsilon: 1.0000
  Buffer Size: 47

Episode 30/10000
  Avg Reward: 1.20/10
  Avg Accuracy: 12.00%
  Avg Loss: 0.0000
  Epsilon: 1.0000
  Buffer Size: 69

Episode 40/10000
  Avg Reward: 0.70/10
  Avg Accuracy: 7.00%
  Avg Loss: 0.0000
  Epsilon: 1.0000
  Buffer Size: 86

Episode 50/10000
  Avg Reward: 0.90/10
  Avg Accuracy: 9.00%
  Avg Loss: 0.0000
  Epsilon: 1.0000
  Buffer Size: 105

============================================================
Evaluating at episode 50...
============================================================
Training:   0%|â–Ž                                                                     | 49/10000 [00:11<40:31,  4.09it/s]
Traceback (most recent call last):
  File "/workspace/train_dqn.py", line 538, in <module>
    rewards, accuracies, losses = train_dqn(
                                  ^^^^^^^^^^
  File "/workspace/train_dqn.py", line 382, in train_dqn
    eval_accuracy = evaluate_agent(env, agent, num_episodes=eval_episodes, verbose=False)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/train_dqn.py", line 429, in evaluate_agent
    action = agent.select_action(state_tensor, epsilon=0.0)  # Greedy
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/train_dqn.py", line 111, in select_action
    q_values = self.q_network(sequence)  # (1, seq_len, vocab_size)
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/model/mini_transformer.py", line 140, in forward
    x = block(x, causal=True)
        ^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/model/mini_transformer.py", line 66, in forward
    attn_output = flash_attn_func(q, k, v, causal=causal)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py", line 1201, in flash_attn_func
    return FlashAttnFunc.apply(
           ^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/autograd/function.py", line 575, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py", line 839, in forward
    out_padded, softmax_lse, S_dmask, rng_state = _wrapped_flash_attn_forward(
                                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/_ops.py", line 1158, in __call__
    return self._op(*args, **(kwargs or {}))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/_library/autograd.py", line 113, in autograd_impl
    result = forward_no_grad(*args, Metadata(keyset, keyword_only_args))
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/_library/autograd.py", line 40, in forward_no_grad
    result = op.redispatch(keyset & _C._after_autograd_keyset, *args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/_ops.py", line 761, in redispatch
    return self._handle.redispatch_boxed(keyset, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/_library/custom_ops.py", line 335, in backend_impl
    result = self._backend_fns[device_type](*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/_compile.py", line 51, in inner
    return disable_fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 838, in _fn
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/_library/custom_ops.py", line 367, in wrapped_fn
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py", line 96, in _flash_attn_forward
    out, softmax_lse, S_dmask, rng_state = flash_attn_gpu.fwd(
                                           ^^^^^^^^^^^^^^^^^^^
RuntimeError: FlashAttention only support fp16 and bf16 data type
